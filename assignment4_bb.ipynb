{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/br3nburk/adleobb/blob/main/assignment4_bb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying Deep Learning to Earth Observation - Assignment 4**"
      ],
      "metadata": {
        "id": "MM6FkL2SjM6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "Work through the assignment 2 notebook, and use this notebook to provide your answers. Be aware that the code cell for `customDataset` in this template is with complete instruction. \n",
        "\n",
        "To submit the assignment, you will need to use GitHub and the existing private repository you already created called `adleoxyz` (xyz is your initials)\n",
        "\n",
        "Once you have completed the assignment:\n",
        "- Commit your notebook from colab to your private GitHub repo\n",
        "- The notebook should be named assignment4_xyz.ipynb, with xyz again replaced by your initials.\n",
        "- There are 50 points in this assignment, with an additional 5 extra credit. "
      ],
      "metadata": {
        "id": "63sSTy-BjbS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical questions**"
      ],
      "metadata": {
        "id": "mU95NKzqm0Lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer to Q1** \n",
        "\n",
        "Using a larger kernel size for conv layers throughout the network:\n",
        "\n",
        "There are two main drawbacks using a larger kernel size for convolutional networks has two main drawbacks. It increases the number of parameters in the network which can cause overfitting. Also, it results in coarser feature maps and thus a loss of spatial resolution.\n",
        "\n",
        "- Increasing the depth of the network by stacking more convolutional layers.\n",
        "\n",
        "This method can lead to vanishing gradients and thus slower training, as well as increased computational complexity and a larger number of parameters.\n",
        "\n",
        "- Downsampling using pooling or convolution with a stride greater than 1.\n",
        "\n",
        "This can lead to a loss in spatial resolution, as the output feature map will be downsampled in comparison to the input.\n",
        "\n",
        "- Using dilated convolutions.\n",
        "\n",
        "This may lead to increased computational complexity, as well as the potential for overfitting due to the increased receptive field size.\n",
        "\n",
        "b)\n",
        "\n",
        "Use smaller dilation rates: Using smaller dilation rates can reduce the computational complexity and time associated with using dilated convolution layers.\n",
        "\n",
        "Use atrous convolution layers: Atrous convolution layers are similar to dilated convolution layers but use a different dilation rate for each kernel, allowing for more efficient use of computational resources.\n",
        "\n",
        "Use spatial pyramid pooling layers: Spatial pyramid pooling layers are an alternative to dilated convolution layers that can reduce computational complexity and time.\n",
        "\n",
        "Use depthwise separable convolution layers: Depthwise separable convolution layers are a type of layer that performs convolution operations in two steps rather than one, resulting in fewer parameters and less computational complexity."
      ],
      "metadata": {
        "id": "J-OwnOYNmudn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Answer to Q2**\n",
        "\n",
        "a) In this case, the effective receptive field of a single neuron in the 2D output of the last conv layer in the block would be 9x9, since all three conv layers have the same kernel size and dilation rate of 1.\n",
        "\n",
        "b) The effective receptive field of a single neuron in the 2D output of the last conv layer in the block would be 17x17, since all three conv layers have the same kernel size and dilation rate of 2.\n",
        "\n",
        "c) The effective receptive field of a single neuron in the 2D output of the last conv layer in the block would be 33x33, since the dilation rates are {\"layer_1\": 2, \"layer_2\":4, \"layer_3\": 8}."
      ],
      "metadata": {
        "id": "Cajkm0yLm7nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer to Q3**\n",
        "\n",
        "\n",
        "The main difference between parallel and serial arrangements of dilated convolution layers is the size and shape of the receptive field. In a parallel arrangement, the size of the receptive field is increased by the sum of the individual dilated convolution layers’ receptive fields, while in a serial arrangement, the size of the receptive field is increased by the product of the individual dilated convolution layers’ receptive fields.\n",
        "\n",
        "The benefits of a parallel arrangement are that it allows for a greater degree of flexibility in the size and shape of the receptive field. Additionally, the parallel arrangement has a lower computational cost than a serial arrangement, since the same input is being used multiple times to create multiple receptive fields.\n",
        "\n",
        "The benefits of a serial arrangement are that it is more efficient in terms of memory usage, as the output of one layer is used as the input to the next layer. Also, a serial arrangement allows for a more precise control over the size and shape of the receptive field, as each layer can have a different dilation rate.\n",
        "\n",
        "The drawbacks of a parallel arrangement are that it can be difficult to control the size and shape of the receptive field, and that it has a higher computational cost.\n",
        "\n",
        "The drawbacks of a serial arrangement are that it can be difficult to increase the size of the receptive field beyond a certain point, and that it requires more memory usage."
      ],
      "metadata": {
        "id": "ZXKsK64_nFyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Continue with our pipeline implementation**"
      ],
      "metadata": {
        "id": "Vz2OmIrBlceE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oiRzE4uliyfk",
        "outputId": "ae44a4a6-4a8c-4e17-a4a2-92fdf8522e03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "2L199Zd1mQmE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import rasterio\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "from IPython.display import Image\n",
        "from natsort import natsorted"
      ],
      "metadata": {
        "id": "G8bnj2T6jZsM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-process the input dataset**"
      ],
      "metadata": {
        "id": "oJtRCLfXlb10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example code is provided below, which will allow you to get up and running with this assignment. However, you will learn best if you use the code you developed/modified from previous assignments to do the work, as you will start to see how it all fits together. "
      ],
      "metadata": {
        "id": "0sxDF0xfnjh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add code for input normalization or use the existing one\n",
        "\n",
        "def min_max_normalize_image(image, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    image_path(str) : Absolute path to the image patch.\n",
        "    dtype (numpy datatype) : data type of the normalized image default is \"np.float32\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the minimum and maximum values for each band\n",
        "    min_values = np.nanmin(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "    max_values = np.nanmax(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "\n",
        "    # Normalize the image data to the range [0, 1]\n",
        "    normalized_img = (image - min_values) / (max_values - min_values)\n",
        "\n",
        "    # Return the normalized image data\n",
        "    return normalized_img"
      ],
      "metadata": {
        "id": "BffGjf8pKDbz",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add the image augmentations functions of your choice here, or use the code below\n",
        "\n",
        "def flip_image_and_label(image, label, flip_type):\n",
        "    \"\"\"\n",
        "    Applies horizontal or vertical flip augmentation to an image patch and label\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        flip_type (string) : Based on the direction of flip. Can be either \n",
        "            'hflip' or 'vflip'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the flipped image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if flip_type == 'hflip':\n",
        "        # Apply horizontal flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 1)\n",
        "        \n",
        "        # Apply horizontal flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 1)\n",
        "        \n",
        "    elif flip_type == 'vflip':\n",
        "        # Apply vertical flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 0)\n",
        "        \n",
        "        # Apply vertical flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 0)\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(\"Flip direction must be 'horizontal' or 'vertical'.\")\n",
        "        \n",
        "    # Return the flipped image patch and label as a tuple\n",
        "    return flipped_image.copy(), flipped_label.copy()\n",
        "\n",
        "\n",
        "def rotate_image_and_label(image, label, angle):\n",
        "    \"\"\"\n",
        "    Applies rotation augmentation to an image patch and label.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        angle (lost of floats) : If the list has exactly two elements they will\n",
        "            be considered the lower and upper bounds for the rotation angle \n",
        "            (in degrees) respectively. If number of elements are bigger than 2, \n",
        "            then one value is chosen randomly as the roatation angle.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the rotated image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(angle, tuple) or isinstance(angle, list):\n",
        "        if len(angle) == 2:\n",
        "            rotation_degree = random.uniform(angle[0], angle[1])\n",
        "        elif len(angle) > 2:\n",
        "            rotation_degree = random.choice(angle)\n",
        "        else:\n",
        "            raise ValueError(\"Parameter degree needs at least two elements.\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Rotation bound param for augmentation must be a tuple or list.\"\n",
        "        )\n",
        "    \n",
        "    # Define the center of the image patch\n",
        "    center = tuple(np.array(label.shape)/2.0)\n",
        "\n",
        "    # Define the rotation matrix\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, rotation_degree, 1.0)\n",
        "\n",
        "    # Apply rotation augmentation to the image patch\n",
        "    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[:2], \n",
        "                                   flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Apply rotation augmentation to the label\n",
        "    rotated_label = cv2.warpAffine(label, rotation_matrix, label.shape[:2], \n",
        "                                   flags=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Return the rotated image patch and label as a tuple\n",
        "    return rotated_image.copy(), np.rint(rotated_label.copy())"
      ],
      "metadata": {
        "id": "O2lQwmu7kWP_",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For assignment 4, you are working with a dataset called \"PondDataset\" which consists pairs of already chipped image and labels of size: `256x256` and pixel values are already in the range of `[0, 1]`. \n",
        "\n",
        "**Structure:**\n",
        "\n",
        "![PondDataset](https://drive.google.com/uc?export=view&id=12Nuy_mVXSAQpnfmpOdycnLBxeEOajMSe)\n",
        "\n",
        "You can find the dataset in the shared drive, which is [here](https://drive.google.com/drive/folders/1hJKRa1tNQmglErELsIEk8hXEykJadmKh?usp=share_link). Please download the entire \"PondDataset\" folder and place it in a convenient locations in your own Google Drive \n",
        "\n",
        "Now you have to adapt your `ActiveLoadingDataset` from reading a \"csv file\", to walk through the folder structure and grab all the \"tiff\" files for \"image\" and \"label\" folders.\n",
        "\n"
      ],
      "metadata": {
        "id": "X7nb42nmkoDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Assignment Part 1\n",
        "\n"
      ],
      "metadata": {
        "id": "_Fj6IwWhnPOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As stated above, you can adapt the code below, or you can use your own loader and adapt it as needed for this assignment. In this case, you need to modify the loader so that it can read chips from a directory, rather than just reading a CSV.  \n",
        "\n"
      ],
      "metadata": {
        "id": "CHBS1s04oFE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add custom dataset from previous assignment and modify to fit the requirements of assignment 4\n",
        "\n",
        "class ActiveLoadingDataset(Dataset):\n",
        "    def __init__(self, src_dir, dataset_name, usage, apply_normalization=False, \n",
        "                 transform=None, **kargs):\n",
        "        r\"\"\"\n",
        "        src_dir (str or path): Root of resource directory.\n",
        "        dataset_name (str): Name of the training/validation dataset containing \n",
        "                              structured folders for image, label\n",
        "        usage (str): Either 'train' or 'validation'.\n",
        "        transform (list): Each element is string name of the transformation to  \n",
        "           be used.\n",
        "        \"\"\"\n",
        "        self.src_dir = src_dir\n",
        "        self.dataset_name = dataset_name\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.usage = usage\n",
        "        assert self.usage in [\"train\", \"validation\"], \"Usage is not recognized.\"\n",
        "\n",
        "        img_dir = Path(src_dir) / self.dataset_name / self.usage / \"images\"\n",
        " \n",
        "        ###### Add your code here. ######\n",
        "        # hint: you need two lines of code or less: \n",
        "        #       2- make a list of filenames for the files with the \".tif\" \n",
        "        #          extension using os.walk and preferably list comprehension\n",
        "        #       3- sort the list of tif files.\n",
        "        \n",
        "        lbl_dir = Path(src_dir) / self.dataset_name / self.usage / \"labels\"\n",
        "\n",
        "        ###### Add your code here. ######\n",
        "        # Do the same for labels.\n",
        "\n",
        "        # Create an empty list to store the TIFF files\n",
        "        lbl_files = []\n",
        "\n",
        "        # use os.walk() to traverse the directory tree\n",
        "        for dirpath, dirnames, filenames in os.walk(lbl_dir):\n",
        "          # iterate over the filenames in the current directory\n",
        "          for filename in filenames:\n",
        "            # check if the file has the specified extension\n",
        "            if filename.lower().endswith(('.tif', '.tiff')):\n",
        "              # If the file is a TIFF file, add it to the list\n",
        "              lbl_files.append(os.path.join(dirpath, filename))\n",
        "\n",
        "              #file_path = os.path.join(dirpath, filename)\n",
        "              #print(file_path)\n",
        "\n",
        "        # Print the list of TIFF files\n",
        "        #print(lbl_files)\n",
        "\n",
        "        lbl_files = natsorted(lbl_files)\n",
        "\n",
        "        img_path = Path(src_dir) / dataset_name / self.usage / 'images'\n",
        "\n",
        "                # Create an empty list to store the TIFF files\n",
        "        img_files = []\n",
        "\n",
        "        # use os.walk() to traverse the directory tree\n",
        "        for dirpath, dirnames, filenames in os.walk(img_path):\n",
        "          # iterate over the filenames in the current directory\n",
        "          for filename in filenames:\n",
        "            # check if the file has the specified extension\n",
        "            if filename.lower().endswith(('.tif', '.tiff')):\n",
        "              # If the file is a TIFF file, add it to the list\n",
        "              img_files.append(os.path.join(dirpath, filename))\n",
        "\n",
        "              file_path = os.path.join(dirpath, filename)\n",
        "              #print(file_path)\n",
        "\n",
        "        #set_trace()\n",
        "        # Print the list of TIFF files\n",
        "        img_files = natsorted(img_files)\n",
        "        print(img_files)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        ##### Add your code here #####\n",
        "        # Hint: iterate through the list of string paths from the image and \n",
        "        # label lists, load the data, transpose image to have channels as the \n",
        "        # last dimension and add them to 'img_chips' and 'lbl_chips' containers.\n",
        "        self.img_chips = []\n",
        "        self.lbl_chips = []\n",
        "\n",
        "        for i in img_files:\n",
        "          with rasterio.open(i) as src:\n",
        "            # Print some metadata about the image\n",
        "            #print(src.profile)\n",
        "            # Access the image data as a numpy array\n",
        "            image_array = src.read()\n",
        "            if apply_normalization==True:\n",
        "                image_array=min_max_normalize_image(image_array)\n",
        "            image_array = image_array.transpose(1,2,0)\n",
        "            self.img_chips.append(image_array)\n",
        "\n",
        "\n",
        "        for i in lbl_files:\n",
        "          with rasterio.open(i) as src:\n",
        "            # Print some metadata about the image\n",
        "            #print(src.profile)\n",
        "            # Access the image data as a numpy array\n",
        "            label_array = src.read(1)\n",
        "            self.lbl_chips.append(label_array)\n",
        "\n",
        "        \n",
        "        print('--------{} patches cropped--------'.format(len(self.img_chips)))\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image_chip = self.img_chips[index]\n",
        "        label_chip = self.lbl_chips[index]\n",
        "\n",
        "        # Change the code if you are using a different augmentation\n",
        "        if self.usage == \"train\" and self.transform:\n",
        "            \n",
        "            trans_flip_ls = [m for m in self.transform if \"flip\" in m]\n",
        "            if random.randint(0, 1) and len(trans_flip_ls) > 1:\n",
        "                trans_flip = random.sample(trans_flip_ls, 1)[0]\n",
        "                image_chip, label_chip = flip_image_and_label(\n",
        "                    image_chip, label_chip, trans_flip\n",
        "                )\n",
        "            \n",
        "            if random.randint(0, 1) and \"rotate\" in self.transform:\n",
        "                img_chip, lbl_chip = rotate_image_and_label(\n",
        "                    image_chip, label_chip, angle=[0,90]\n",
        "                )\n",
        "\n",
        "        # Convert numpy arrays to torch tensors.\n",
        "        # Image chips should be in 'CHW' order, if not transpose to correct \n",
        "        # order of dimensions.\n",
        "        ##### Add code here #####\n",
        "        # create the two tensors here for the output below, using the correct \n",
        "        # reshaping functions\n",
        "        image_tensor = torch.from_numpy(image_chip.transpose((2, 0, 1))).float()\n",
        "        label_tensor = torch.from_numpy(np.ascontiguousarray(label_chip)).long()\n",
        "\n",
        "\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.img_chips)"
      ],
      "metadata": {
        "id": "n1SNASS_kD2u",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading your data**"
      ],
      "metadata": {
        "id": "j2xVbJEqy2KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/A4\"\n",
        "dataset_name = \"PondDataset\"\n",
        "usage = 'train'\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.1\n",
        "\n",
        "criterion1 = \"BalancedCrossEntropyLoss()\"\n",
        "criterion2= \"DiceLoss()\"\n",
        "WorkingFolder1 = \"/content/gdrive/MyDrive/A4/Test2/Model1\"\n",
        "WorkingFolder2 = \"/content/gdrive/MyDrive/A4/Test2/Model2\"\n",
        "initial_lr = 0.01\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "OKYZ7Cb9y9e4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_dataset = ActiveLoadingDataset(src_dir, dataset_name, usage='train', \n",
        "                                     transform=transform)"
      ],
      "metadata": {
        "id": "5dGykQgXzDl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f913e95-1755-4682-a8fb-bc9823ae6200"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip1.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip2.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip3.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip4.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip5.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip7.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip8.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip12.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip13.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip14.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip15.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip16.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip17.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip18.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip19.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip20.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip21.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip22.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip23.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip24.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip25.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip26.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip27.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip28.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip29.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip30.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip31.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip34.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip35.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip36.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip37.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip38.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip39.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip40.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip41.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip42.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip43.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip44.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip45.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip46.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip47.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip48.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip49.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip50.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip51.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip52.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip53.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip54.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip58.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip59.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip60.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip61.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip62.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip63.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip64.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip65.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip66.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip67.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip68.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip69.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip70.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip71.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip72.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip73.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip74.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip75.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip77.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip78.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip79.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip80.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip81.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip82.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip83.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip84.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip85.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip87.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip88.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip89.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip90.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip91.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip92.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip93.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip95.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip96.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip97.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip98.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip99.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip100.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip101.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip102.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip103.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip104.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip105.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip106.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip107.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip108.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip109.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip110.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip111.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip112.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip113.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip114.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip115.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip116.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip117.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip118.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip119.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip120.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip121.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip122.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip123.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip124.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip125.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip126.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip127.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip128.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip129.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip131.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip132.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip133.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip134.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip136.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip137.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip138.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip139.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip140.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip141.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip142.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip143.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip144.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip145.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip146.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip147.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip148.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip150.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip151.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip152.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip153.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip154.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip155.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip156.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip158.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip159.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip160.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip161.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip162.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip168.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip169.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip170.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip171.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip172.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip173.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip174.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip175.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip177.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip178.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip179.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip180.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip181.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip182.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip183.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip184.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip185.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip186.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip187.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip197.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip198.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip199.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip200.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip201.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip204.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip205.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip206.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip209.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip210.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip211.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip212.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip213.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip214.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip215.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip216.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip217.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip218.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip219.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip220.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip221.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip223.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip224.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip225.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip226.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip227.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip228.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip229.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip230.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip231.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip232.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip233.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip234.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip235.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip236.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip237.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip238.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip239.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip240.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip241.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip242.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip244.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip245.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip246.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip247.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip248.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip249.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip250.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip251.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip252.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip253.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip254.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip255.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip256.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip257.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip258.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip264.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip265.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip266.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip267.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip268.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip269.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip12.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip13.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip52.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip53.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip55.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip56.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip69.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip70.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip72.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip73.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip74.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip75.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip77.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip78.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip79.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip81.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip82.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip98.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip99.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip100.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip113.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip114.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip117.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip118.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip121.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip122.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip123.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip124.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip125.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip126.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip127.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip130.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip131.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip132.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip134.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip144.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip147.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip148.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip149.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip150.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip152.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip153.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip154.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip156.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip157.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip158.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip159.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip160.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip161.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip162.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip163.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip164.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip165.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip166.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip167.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip168.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip169.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip170.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip171.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip40.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip41.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip42.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip46.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip47.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip50.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip51.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip106.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip121.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip122.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip123.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip124.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip420.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip460.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip461.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip462.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip463.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip464.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip492.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip498.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip499.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip507.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip508.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip509.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip510.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip511.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip512.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip513.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip514.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip517.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip518.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip519.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip520.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip521.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip522.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip523.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip525.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip526.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip529.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip530.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip531.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip535.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip540.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip541.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip329.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip330.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip331.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip332.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip341.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip342.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip343.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip344.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip345.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip346.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip352.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip353.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip354.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip355.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip356.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip357.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip365.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip366.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip367.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip368.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip369.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip370.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip371.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip381.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip382.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip383.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip384.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip385.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip386.tif']\n",
            "--------359 patches cropped--------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 16, \n",
        "                          shuffle = True)"
      ],
      "metadata": {
        "id": "5j3ElFJWzKw8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = ActiveLoadingDataset(src_dir, dataset_name, \n",
        "                                          usage=\"validation\", \n",
        "                                          apply_normalization=False)"
      ],
      "metadata": {
        "id": "WFJLUAfmzOlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d8f82f-2416-486b-9b65-32e019620820"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip14.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip18.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip21.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip28.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip29.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip32.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip33.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip34.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip43.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip44.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip56.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip57.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip58.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip60.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip83.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip84.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip85.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip87.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip88.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip89.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip90.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip91.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip961.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip962.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip963.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip964.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip998.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip999.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1001.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1002.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1003.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1036.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1037.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1063.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1064.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1076.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1077.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1161.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1162.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1163.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1167.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1168.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1169.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1170.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1171.tif']\n",
            "--------45 patches cropped--------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(validation_dataset, batch_size = 1, shuffle = False)"
      ],
      "metadata": {
        "id": "pUMP12gmzVkT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model architecture**"
      ],
      "metadata": {
        "id": "X4Zo8L59l6y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For your reference I have added a complete Unet architecture."
      ],
      "metadata": {
        "id": "RCIIyjCM4B5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add the Unet model you have designed from assignment 3 or use the existing one.\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    r\"\"\"This module creates a user-defined number of conv+BN+ReLU layers.\n",
        "    Args:\n",
        "        in_channels (int)-- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        kernel_size (int) -- Size of convolution kernel.\n",
        "        stride (int) -- decides how jumpy kernel moves along the spatial \n",
        "            dimensions.\n",
        "        padding (int) -- how much the input should be padded on the borders with \n",
        "            zero.\n",
        "        dilation (int) -- dilation ratio for enlarging the receptive field.\n",
        "        num_conv_layers (int) -- Number of conv+BN+ReLU layers in the block.\n",
        "        drop_rate (float) -- dropout rate at the end of the block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
        "                 padding=1, dilation=1, num_conv_layers=2, drop_rate=0):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                            stride=stride, padding=padding, dilation=dilation, \n",
        "                            bias=False),\n",
        "                  nn.BatchNorm2d(out_channels),\n",
        "                  nn.ReLU(inplace=True), ]\n",
        "\n",
        "        if num_conv_layers > 1:\n",
        "            if drop_rate > 0:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels, \n",
        "                              kernel_size=kernel_size, stride=stride, \n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
        "                    nn.Dropout(drop_rate), \n",
        "                ] * (num_conv_layers - 1)\n",
        "            else:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels, \n",
        "                              kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), \n",
        "                ] * (num_conv_layers - 1)\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.block(inputs)\n",
        "        return outputs\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class UpconvBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Decoder layer decodes the features along the expansive path.\n",
        "    Args:\n",
        "        in_channels (int) -- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        upmode (str) -- Upsampling type. If \"fixed\" then a linear upsampling with scale factor\n",
        "                        of two will be applied using bi-linear as interpolation method.\n",
        "                        If deconv_1 is chosen then a non-overlapping transposed convolution will\n",
        "                        be applied to upsample the feature maps. If deconv_1 is chosen then an\n",
        "                        overlapping transposed convolution will be applied to upsample the feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, upmode=\"deconv_1\"):\n",
        "        super(UpconvBlock, self).__init__()\n",
        "\n",
        "        if upmode == \"fixed\":\n",
        "            layers = [nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True), ]\n",
        "            layers += [nn.BatchNorm2d(in_channels),\n",
        "                       nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False), ]\n",
        "\n",
        "        elif upmode == \"deconv_1\":\n",
        "            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0, dilation=1), ]\n",
        "\n",
        "        elif upmode == \"deconv_2\":\n",
        "            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, dilation=1), ]\n",
        "\n",
        "        # Dense Upscaling Convolution\n",
        "        elif upmode == \"DUC\":\n",
        "            up_factor = 2\n",
        "            upsample_dim = (up_factor ** 2) * out_channels\n",
        "            layers = [nn.Conv2d(in_channels, upsample_dim, kernel_size=3, padding=1),\n",
        "                      nn.BatchNorm2d(upsample_dim),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.PixelShuffle(up_factor), ]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Provided upsampling mode is not recognized.\")\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.block(inputs)\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, n_classes, in_channels, filter_config=None, dropout_rate=0):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        if not filter_config:\n",
        "            filter_config = (64, 128, 256, 512, 1024, 2048)\n",
        "\n",
        "        assert len(filter_config) == 6\n",
        "\n",
        "        # Contraction Path\n",
        "        self.encoder_1 = ConvBlock(self.in_channels, filter_config[0], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 64x256x256\n",
        "        self.encoder_2 = ConvBlock(filter_config[0], filter_config[1], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 128x128x128\n",
        "        self.encoder_3 = ConvBlock(filter_config[1], filter_config[2], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 256x64x64\n",
        "        self.encoder_4 = ConvBlock(filter_config[2], filter_config[3], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 512x32x32\n",
        "        self.encoder_5 = ConvBlock(filter_config[3], filter_config[4], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 1024x16x16\n",
        "        self.encoder_6 = ConvBlock(filter_config[4], filter_config[5], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 2048x8x8\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Expansion Path\n",
        "        self.decoder_1 = UpconvBlock(filter_config[5], filter_config[4], upmode=\"deconv_2\")  # 1024x16x16\n",
        "        self.conv1 = ConvBlock(filter_config[4] * 2, filter_config[4], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_2 = UpconvBlock(filter_config[4], filter_config[3], upmode=\"deconv_2\")  # 512x32x32\n",
        "        self.conv2 = ConvBlock(filter_config[4], filter_config[3], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_3 = UpconvBlock(filter_config[3], filter_config[2], upmode=\"deconv_2\")  # 256x64x64\n",
        "        self.conv3 = ConvBlock(filter_config[3], filter_config[2], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_4 = UpconvBlock(filter_config[2], filter_config[1], upmode=\"deconv_2\")  # 128x128x128\n",
        "        self.conv4 = ConvBlock(filter_config[2], filter_config[1], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_5 = UpconvBlock(filter_config[1], filter_config[0], upmode=\"deconv_2\")  # 64x256x256\n",
        "        self.conv5 = ConvBlock(filter_config[1], filter_config[0], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.classifier = nn.Conv2d(filter_config[0], n_classes, kernel_size=1, stride=1, padding=0)  # classNumx256x256\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # set_trace()\n",
        "        e1 = self.encoder_1(inputs)  # batch size x 64 x 256 x 256\n",
        "        p1 = self.pool(e1)  # batch size x 64 x 128 x 128\n",
        "\n",
        "        e2 = self.encoder_2(p1)  # batch size x 128 x 128 x 128\n",
        "        p2 = self.pool(e2)  # batch size x 128 x 64 x 64\n",
        "\n",
        "        e3 = self.encoder_3(p2)  # batch size x 256 x 64 x 64\n",
        "        p3 = self.pool(e3)  # batch size x 256 x 32 x 32\n",
        "\n",
        "        e4 = self.encoder_4(p3)  # batch size x 512 x 32 x 32\n",
        "        p4 = self.pool(e4)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        e5 = self.encoder_5(p4)  # batch size x 1024 x 16 x 16\n",
        "        p5 = self.pool(e5)  # batch size x 1024 x 8 x 8\n",
        "\n",
        "        e6 = self.encoder_6(p5)  # batch size x 2048 x 8 x 8\n",
        "\n",
        "        d6 = self.decoder_1(e6)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        \n",
        "        skip1 = torch.cat((e5, d6), dim=1)  # batch size x 2048 x 16 x 16\n",
        "\n",
        "        d6_proper = self.conv1(skip1)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        d5 = self.decoder_2(d6_proper)  # batch size x 512 x 32 x 32\n",
        "\n",
        "        skip2 = torch.cat((e4, d5), dim=1)  # batch size x 1024 x 32 x 32\n",
        "\n",
        "        d5_proper = self.conv2(skip2)  # batch size x 512 x 32 x 32\n",
        "\n",
        "        d4 = self.decoder_3(d5_proper)  # batch size x 256 x 64 x 64\n",
        "\n",
        "        skip3 = torch.cat((e3, d4), dim=1)  # batch size x 512 x 64 x 64\n",
        "\n",
        "        d4_proper = self.conv3(skip3)  # batch size x 256 x 64 x 64\n",
        "\n",
        "        d3 = self.decoder_4(d4_proper)  # batch size x 128 x 128 x 128\n",
        "\n",
        "        skip4 = torch.cat((e2, d3), dim=1)  # batch size x 256 x 128 x 128\n",
        "\n",
        "        d3_proper = self.conv4(skip4)  # batch size x 128 x 128 x 128\n",
        "\n",
        "        d2 = self.decoder_5(d3_proper)  # batch size x 64 x 256 x 256\n",
        "\n",
        "        skip5 = torch.cat((e1, d2), dim=1)  # batch size x 128 x 256 x 256\n",
        "\n",
        "        d2_proper = self.conv5(skip5)  # batch size x 64 x 256 x 256\n",
        "\n",
        "        d1 = self.classifier(d2_proper)  # batch size x classNum x 256 x 256\n",
        "\n",
        "        return d1"
      ],
      "metadata": {
        "id": "syq2_DKrmBoL",
        "cellView": "form"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing your model**"
      ],
      "metadata": {
        "id": "YWdmgmPIzeE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.15"
      ],
      "metadata": {
        "id": "91DbioGYzluC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(n_classes, in_channels, filter_config, dropout_rate)"
      ],
      "metadata": {
        "id": "Qtiayi6AzoA-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Customized loss function**\n",
        "\n",
        "You will want to add two here, which you can copy from the main assignment notebook. "
      ],
      "metadata": {
        "id": "P4hMpDQ1ma0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add your choices of loss function here \n",
        "\n",
        "class BalancedCrossEntropyLoss(nn.Module):\n",
        "    '''\n",
        "    Balanced cross entropy loss by weighting of inverse class ratio\n",
        "    Params:\n",
        "        ignore_index (int): Class index to ignore\n",
        "        reduction (str): Reduction method to apply to loss, return mean over batch if 'mean',\n",
        "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "    Returns:\n",
        "        Loss tensor according to arg reduction\n",
        "    '''\n",
        "\n",
        "    def __init__(self, ignore_index=-100, reduction='mean'):\n",
        "        super(BalancedCrossEntropyLoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # get class weights\n",
        "        class_counts = torch.bincount(target.view(-1), minlength=predict.shape[1])\n",
        "        class_weights = 1.0 / torch.sqrt(class_counts.float())\n",
        "\n",
        "        # set weight of ignore index to 0\n",
        "        if self.ignore_index >= 0 and self.ignore_index < len(class_weights):\n",
        "            class_weights[self.ignore_index] = 0\n",
        "\n",
        "        # normalize weights\n",
        "        class_weights /= torch.sum(class_weights)\n",
        "\n",
        "        # apply class weights to loss function\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=self.ignore_index, reduction=self.reduction)\n",
        "\n",
        "        return loss_fn(predict, target)\n",
        "\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "    '''\n",
        "        Dice loss of binary class\n",
        "        Params:\n",
        "            smooth (float): A float number to smooth loss, and avoid NaN error, default: 1\n",
        "            p (int): Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2. Used\n",
        "                     to control the sensitivity of the loss.\n",
        "            predict (torch.tensor): Predicted tensor of shape [N, *]\n",
        "            target (torch.tensor): Target tensor of same shape with predict\n",
        "        Returns:\n",
        "            Loss tensor\n",
        "    '''\n",
        "    def __init__(self, smooth=1, p=1):\n",
        "        super(BinaryDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.p = p\n",
        "    #set_trace()\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        #set_trace()\n",
        "\n",
        "        assert predict.shape == target.shape, \"predict & target shape do not match\"\n",
        "        predict = predict.contiguous().view(-1)\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        num = 2 * (predict * target).sum() + self.smooth\n",
        "        den = (predict.pow(self.p) + target.pow(self.p)).sum() + self.smooth\n",
        "        loss = 1 - num / den\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    '''\n",
        "        Dice loss\n",
        "        Params:\n",
        "            weight (torch.tensor): Weight array of shape [num_classes,]\n",
        "            ignore_index (int): Class index to ignore\n",
        "            predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "            target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
        "            reduction (str): Reduction method.\n",
        "        Returns:\n",
        "            same as BinaryDiceLoss\n",
        "    '''\n",
        "    def __init__(self, weight=None, ignore_index=-100, smooth=1, p=1, reduction='sum'):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "        self.dice = BinaryDiceLoss(smooth, p)\n",
        "        if weight is not None:\n",
        "            self.weight = weight.cuda()\n",
        "        else:\n",
        "            self.weight = None\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        #set_trace()\n",
        "        if predict.shape == target.shape:\n",
        "            pass\n",
        "        elif len(predict.shape) == 4:\n",
        "            target = F.one_hot(target, num_classes=predict.shape[1]).permute(0, 3, 1, 2).contiguous()\n",
        "        else:\n",
        "            assert 'predict shape not applicable'\n",
        "\n",
        "        self.weight = torch.Tensor([1. / predict.shape[1]] * predict.shape[1]).cuda() if self.weight is None else self.weight\n",
        "        predict = F.softmax(predict, dim=1)\n",
        "\n",
        "        if self.ignore_index >= 0:\n",
        "            target = torch.where(target == self.ignore_index, torch.zeros_like(target), target)\n",
        "            predict = torch.where(target == self.ignore_index, torch.zeros_like(predict), predict)\n",
        "\n",
        "        total_loss = 0\n",
        "        for i in range(predict.shape[1]):\n",
        "            dice_loss = self.dice(predict[:, i], target[:, i])\n",
        "\n",
        "            assert self.weight.shape[0] == predict.shape[1], \\\n",
        "                    'Expect weight shape [{}], get[{}]'.format(predict.shape[1], self.weight.shape[0])\n",
        "            dice_loss *= self.weight[i]\n",
        "            total_loss += dice_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            loss = total_loss / predict.shape[1]\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = total_loss\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "cellView": "form",
        "id": "H8fe9coXmgOh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coding assignment part 2**: training the network\n",
        "\n"
      ],
      "metadata": {
        "id": "8M9vkOj7AT86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the sections below you need to complete the three functions that you need to train and validate the network over a specified number of epochs  "
      ],
      "metadata": {
        "id": "eh13sfNJvDJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HOW To:**\n",
        "\n",
        "Properly use the \"criterion\" argument inside the \"train\" and \"validation\" functions:\n",
        "Pass the argument to the function as a string with `()` like: \"BalancedCrossEntropyLoss()\"\n",
        "\n",
        "Now as you try to use the argument inside both \"train\" and \"validation\" functions, use `eval()` like: \n",
        "\n",
        "`loss = eval(criterion)(tensor A, tensor B)`\n",
        "\n",
        "**Note:**\n",
        "\n",
        "`eval()` is a built-in Python function that allows you to evaluate a string expression as a Python code. It takes a string as an argument and evaluates the expression contained in it. The result of the evaluation is then returned."
      ],
      "metadata": {
        "id": "KWRrySwHBBT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Complete the function to optimize over a batch of training images and labels\n",
        "def train(trainData, model, optimizer, criterion, gpu=True, train_loss=[]):\n",
        "    \"\"\"\n",
        "        Train the model using provided training dataset.\n",
        "        Params:\n",
        "            trainData (DataLoader object) -- Batches of image chips from PyTorch \n",
        "                custom dataset (AquacultureData).\n",
        "            model -- Choice of segmentation model.\n",
        "            optimizer -- Chosen optimization algorithm to update model parameters.\n",
        "            criterion -- Chosen function to calculate loss over training samples.\n",
        "            gpu (bool, optional) -- Decide whether to use GPU, default is True.\n",
        "            train_loss (empty list, optional) -- ???????????????????????????\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Mini batch iteration\n",
        "    train_epoch_loss = 0\n",
        "    train_batches = len(trainData)\n",
        "\n",
        "    for img_chips, labels in trainData:\n",
        "\n",
        "        #Add code to put image and label on the 'device'.\n",
        "        # one line for each.\n",
        "\n",
        "        # Add code to clear the 'optimizer' from existing gradients (1 line)\n",
        "        # Pass image through the model to obtain prediction (1 line)\n",
        "        # calculate loss based on 'model prediction' and label (1 line)\n",
        "        # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "        # do the backward pass to calculate gradients with respect to the loss (1 line)\n",
        "        # update model weights by invoking the proper method on 'optimizer'\n",
        "\n",
        "        img = img_chips.to(device)\n",
        "        label = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(img)\n",
        "\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        train_epoch_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss.append(train_epoch_loss / train_batches)\n",
        "    print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches))"
      ],
      "metadata": {
        "id": "g-AEldksAikB",
        "cellView": "form"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides training the network, it's important to evaluate its performance on a separate \"validation dataset\" to ensure that it's not overfitting to the training data. The validation process is similar to the training process, but the network is set to evaluation mode using `model.eval()` and the gradients are not computed."
      ],
      "metadata": {
        "id": "HvQWD_wTs6lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Complete the function to iterate validation images and labels\n",
        "\n",
        "\n",
        "def validate(valData, model, criterion, device, val_loss=[]):\n",
        "    \"\"\"\n",
        "        Evaluate the model on separate Landsat scenes.\n",
        "        Params:\n",
        "            valData (DataLoader object) -- Batches of image chips from PyTorch custom dataset(AquacultureData)\n",
        "            model -- Choice of segmentation Model.\n",
        "            criterion -- Chosen function to calculate loss over validation samples.\n",
        "            buffer: Buffer added to the targeted grid when creating dataset. This allows loss to calculate\n",
        "                at non-buffered region.\n",
        "            gpu (binary,optional): Decide whether to use GPU, default is True\n",
        "            valLoss (empty list): To record average loss for each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # mini batch iteration\n",
        "    eval_epoch_loss = 0\n",
        "\n",
        "    for img_chips, labels in valData:\n",
        "\n",
        "        img = Variable(img_chips, requires_grad=False)\n",
        "        label = Variable(labels, requires_grad=False)\n",
        "\n",
        "        #Add code to put image and label on the 'device'.\n",
        "        # one line for each.\n",
        "\n",
        "        img = img_chips.to(device)\n",
        "        label = labels.to(device)\n",
        "\n",
        "        pred = model(img)\n",
        "\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        eval_epoch_loss += loss.item()\n",
        "\n",
        "        # Pass image through the model to obtain prediction (1 line)\n",
        "        # calculate loss based on 'model prediction' and label (1 line)\n",
        "        # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "\n",
        "    print('validation loss: {}'.format(eval_epoch_loss / len(valData)))\n",
        "\n",
        "    if val_loss != None:\n",
        "        val_loss.append(float(eval_epoch_loss / len(valData)))\n"
      ],
      "metadata": {
        "id": "l2VxqLHiBDFi",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Complete the function to iterate over the desired number of epochs\n",
        "\n",
        "def epochIterater(trainData, valData, model, criterion, WorkingFolder, initial_lr, num_epochs):\n",
        "    r\"\"\"\n",
        "    Epoch iteration for train and evaluation.\n",
        "    \n",
        "    Arguments:\n",
        "    trainData (dataloader object): Batch grouped data to train the model.\n",
        "    evalData (dataloader object): Batch grouped data to evaluate the model.\n",
        "    model (pytorch.nn.module object): initialized model.\n",
        "    initial_lr(float): The initial learning rate.\n",
        "    num_epochs (int): User-defined number of epochs to run the model.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == \"cuda\":\n",
        "        print('----------GPU available----------')\n",
        "        gpu = True\n",
        "        model = model.to(device)\n",
        "    \n",
        "    else:\n",
        "        print('----------No GPU available, using CPU instead----------')\n",
        "        gpu = False\n",
        "        model = model\n",
        "    \n",
        "    writer = SummaryWriter(WorkingFolder)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=initial_lr,\n",
        "                           betas=(0.9, 0.999),\n",
        "                           eps=1e-08,\n",
        "                           weight_decay=5e-4,\n",
        "                           amsgrad=False)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "                                          step_size=3,\n",
        "                                          gamma=0.98)\n",
        "    \n",
        "    # Add your code here\n",
        "    # you need to loop through the epochs and perform the following:\n",
        "    # print the current epoch number out of the total epochs (e.g. \"epoch: 2/10\")(1 line)\n",
        "    # start the timer (1 line)\n",
        "    # do model fit on the training data for single epoch (1 line)\n",
        "    # do model validation on the validation dataset for one epoch (1 line)\n",
        "    # take a step to update the 'scheduler'. (1 line)\n",
        "    # Print the updated learning rate.\n",
        "    # use \"add_scalars\" method with your writer to save the train and validation loss to graph\n",
        "    # using tensorboard package later. \n",
        "    \n",
        "    for t in range(num_epochs):\n",
        "        print(\"Epoch [{}/{}]\".format(t + 1, num_epochs))\n",
        "        start_epoch = datetime.now()\n",
        "\n",
        "        train(trainData, model, optimizer, criterion, device, train_loss=train_loss)\n",
        "        validate(valData, model, criterion, device, val_loss=val_loss)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(\"LR: {}\".format(scheduler.get_last_lr()))\n",
        "\n",
        "        writer.add_scalars(\"Loss\", \n",
        "                           {\"train loss\": train_loss[t],\n",
        "                            \"validation loss\": val_loss[t]},\n",
        "                           t + 1)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"--------------- Training finished in {} ---------------\".format(duration_format))"
      ],
      "metadata": {
        "id": "TQFVUtNtD2s_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstrate the code\n",
        "Run the model training and validation for a specified number of epochs (e.g. 15), and then save the results. Train / validate twice, once using your first loss function, and again using your second loss function.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pG9Ni2kwKC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train/validate 1\n",
        "src_dir = \"/content/gdrive/MyDrive/A4\"\n",
        "dataset_name = \"PondDataset\"\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.1\n",
        "\n",
        "criterion1 = \"BalancedCrossEntropyLoss()\"\n",
        "criterion2= \"DiceLoss()\"\n",
        "WorkingFolder1 = \"/content/gdrive/MyDrive/A4/Test2/Model1\"\n",
        "WorkingFolder2 = \"/content/gdrive/MyDrive/Clark/A4/Test2/Model2\"\n",
        "initial_lr = 0.01\n",
        "num_epochs = 20\n",
        "\n",
        "trainData = ActiveLoadingDataset(src_dir, dataset_name, usage=\"train\", \n",
        "                 transform=transform)\n",
        "\n",
        "valData = ActiveLoadingDataset(src_dir, dataset_name, usage=\"validation\", transform=transform)\n",
        "\n",
        "epochIterater(train_loader, val_loader, model, criterion1, \n",
        "              WorkingFolder=WorkingFolder1, initial_lr=initial_lr, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "wq1-0zAKyLQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc07a8e3-aef9-4ec5-9ed0-d67c57e4d35a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip1.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip2.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip3.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip4.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip5.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip7.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip8.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip12.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip13.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip14.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip15.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip16.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip17.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip18.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip19.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip20.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip21.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip22.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip23.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip24.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip25.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip26.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip27.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip28.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip29.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip30.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip31.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip34.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip35.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip36.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip37.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip38.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip39.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip40.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip41.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip42.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip43.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip44.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip45.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip46.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip47.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip48.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip49.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip50.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip51.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip52.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip53.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip54.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip58.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip59.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip60.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip61.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip62.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip63.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip64.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip65.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip66.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip67.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip68.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip69.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip70.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip71.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip72.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip73.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip74.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip75.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip77.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip78.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip79.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip80.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip81.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip82.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip83.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip84.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip85.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip87.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip88.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip89.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip90.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip91.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip92.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip93.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip95.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip96.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip97.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip98.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip99.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip100.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip101.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip102.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip103.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip104.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip105.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip106.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip107.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip108.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip109.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip110.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip111.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip112.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip113.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip114.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip115.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip116.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip117.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip118.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip119.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip120.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip121.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip122.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip123.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip124.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip125.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip126.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip127.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip128.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip129.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip131.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip132.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip133.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip134.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip136.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip137.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip138.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip139.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip140.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip141.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip142.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip143.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip144.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip145.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip146.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip147.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip148.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip150.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip151.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip152.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip153.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip154.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip155.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip156.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip158.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip159.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip160.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip161.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip162.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip168.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip169.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip170.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip171.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip172.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip173.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip174.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip175.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip177.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip178.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip179.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip180.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip181.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip182.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip183.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip184.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip185.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip186.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip187.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip197.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip198.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip199.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip200.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip201.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip204.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip205.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip206.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip209.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip210.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip211.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip212.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip213.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip214.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip215.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip216.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip217.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip218.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip219.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip220.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip221.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip223.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip224.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip225.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip226.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip227.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip228.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip229.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip230.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip231.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip232.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip233.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip234.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip235.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip236.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip237.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip238.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip239.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip240.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip241.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip242.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip244.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip245.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip246.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip247.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip248.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip249.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip250.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip251.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip252.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip253.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip254.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip255.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip256.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip257.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip258.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip264.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip265.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip266.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip267.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip268.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip269.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip12.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip13.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip52.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip53.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip55.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip56.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip69.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip70.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip72.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip73.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip74.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip75.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip77.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip78.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip79.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip81.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip82.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip98.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip99.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip100.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip113.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip114.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip117.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip118.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip121.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip122.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip123.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip124.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip125.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip126.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip127.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip130.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip131.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip132.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip134.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip144.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip147.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip148.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip149.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip150.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip152.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip153.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip154.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip156.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip157.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip158.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip159.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip160.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip161.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip162.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip163.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip164.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip165.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip166.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip167.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip168.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip169.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip170.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_116060_20190221_20190221_01_RT_band_chip171.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip40.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip41.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip42.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip46.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip47.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip50.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip51.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip106.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip121.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip122.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip123.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip124.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip420.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip460.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip461.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip462.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip463.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip464.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip492.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip498.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip499.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip507.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip508.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip509.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip510.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip511.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip512.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip513.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip514.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip517.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip518.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip519.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip520.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip521.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip522.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip523.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip525.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip526.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip529.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip530.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip531.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip535.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip540.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_126045_20181006_20181010_01_T1_band_chip541.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip329.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip330.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip331.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip332.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip341.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip342.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip343.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip344.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip345.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip346.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip352.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip353.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip354.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip355.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip356.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip357.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip365.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip366.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip367.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip368.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip369.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip370.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip371.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip381.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip382.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip383.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip384.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip385.tif', '/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_128051_20180206_20180221_01_T1_band_chip386.tif']\n",
            "--------359 patches cropped--------\n",
            "['/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip14.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip18.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip21.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip28.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip29.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip32.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip33.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip34.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip43.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip44.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip56.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip57.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip58.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip60.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip83.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip84.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip85.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip87.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip88.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip89.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip90.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_011062_20171013_20171024_01_T1_band_chip91.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip961.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip962.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip963.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip964.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip998.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip999.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1001.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1002.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1003.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1036.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1037.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1063.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1064.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1076.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1077.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1161.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1162.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1163.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1167.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1168.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1169.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1170.tif', '/content/gdrive/MyDrive/A4/PondDataset/validation/images/LC08_L1TP_129051_20180213_20180222_01_T1_band_chip1171.tif']\n",
            "--------45 patches cropped--------\n",
            "----------No GPU available, using CPU instead----------\n",
            "Epoch [1/20]\n",
            "Training loss: 0.4781\n",
            "validation loss: 2.8041394015153247\n",
            "LR: [0.01]\n",
            "Epoch [2/20]\n",
            "Training loss: 0.3601\n",
            "validation loss: 0.49485400981373256\n",
            "LR: [0.01]\n",
            "Epoch [3/20]\n",
            "Training loss: 0.3417\n",
            "validation loss: 1.0656105019980007\n",
            "LR: [0.0098]\n",
            "Epoch [4/20]\n",
            "Training loss: 0.2832\n",
            "validation loss: 0.4605311604009734\n",
            "LR: [0.0098]\n",
            "Epoch [5/20]\n",
            "Training loss: 0.2695\n",
            "validation loss: 0.6558590938647588\n",
            "LR: [0.0098]\n",
            "Epoch [6/20]\n",
            "Training loss: 0.2685\n",
            "validation loss: 1.16148694306612\n",
            "LR: [0.009604]\n",
            "Epoch [7/20]\n",
            "Training loss: 0.2671\n",
            "validation loss: 0.48803262909253436\n",
            "LR: [0.009604]\n",
            "Epoch [8/20]\n",
            "Training loss: 0.2986\n",
            "validation loss: 0.5961423469914331\n",
            "LR: [0.009604]\n",
            "Epoch [9/20]\n",
            "Training loss: 0.2350\n",
            "validation loss: 0.6344362241526444\n",
            "LR: [0.009411919999999999]\n",
            "Epoch [10/20]\n",
            "Training loss: 0.2457\n",
            "validation loss: 0.43108553306923975\n",
            "LR: [0.009411919999999999]\n",
            "Epoch [11/20]\n",
            "Training loss: 0.2235\n",
            "validation loss: 0.4687991784678565\n",
            "LR: [0.009411919999999999]\n",
            "Epoch [12/20]\n",
            "Training loss: 0.2242\n",
            "validation loss: 0.48513836595747206\n",
            "LR: [0.009223681599999999]\n",
            "Epoch [13/20]\n",
            "Training loss: 0.2235\n",
            "validation loss: 0.6691170589791404\n",
            "LR: [0.009223681599999999]\n",
            "Epoch [14/20]\n",
            "Training loss: 0.2203\n",
            "validation loss: 0.4386397378312217\n",
            "LR: [0.009223681599999999]\n",
            "Epoch [15/20]\n",
            "Training loss: 0.2027\n",
            "validation loss: 0.48535607788297863\n",
            "LR: [0.009039207967999998]\n",
            "Epoch [16/20]\n",
            "Training loss: 0.2113\n",
            "validation loss: 0.46821842160489824\n",
            "LR: [0.009039207967999998]\n",
            "Epoch [17/20]\n",
            "Training loss: 0.2163\n",
            "validation loss: 0.5949594804810153\n",
            "LR: [0.009039207967999998]\n",
            "Epoch [18/20]\n",
            "Training loss: 0.2255\n",
            "validation loss: 0.5238828155729506\n",
            "LR: [0.008858423808639998]\n",
            "Epoch [19/20]\n",
            "Training loss: 0.2118\n",
            "validation loss: 0.8149939722485012\n",
            "LR: [0.008858423808639998]\n",
            "Epoch [20/20]\n",
            "Training loss: 0.1886\n",
            "validation loss: 0.42592437035507624\n",
            "LR: [0.008858423808639998]\n",
            "--------------- Training finished in 0:17:59 ---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save model results 1 in a directory of choice in your gdrive\n",
        "torch.save(model.state_dict(), \n",
        "           os.path.join(Path(WorkingFolder1), \"A4_1.pth\"))"
      ],
      "metadata": {
        "id": "bLd8KNWID22z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "acd1815f-ffb1-45bb-9622-53995079128f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9d9eb0db58fb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Save model results 1 in a directory of choice in your gdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = Unet(n_classes, \n\u001b[0m\u001b[1;32m      3\u001b[0m              \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mfilter_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              dropout_rate)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Unet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train/validate 2"
      ],
      "metadata": {
        "id": "P9zRPuuCyZlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save model results 1 in a directory of choice in your gdrive\n",
        "torch.save(model.state_dict(), \n",
        "           os.path.join(Path(WorkingFolder2), \"A4_2.pth\"))"
      ],
      "metadata": {
        "id": "kyrDM0hqyeMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation and accuracy metrics**"
      ],
      "metadata": {
        "id": "FaCaNlvonXp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\n",
        "If you have disconnected from the Colab session or restarted the kernel, then before doing the evaluation on the validation dataset you must initialize your model once more and load the trained weights onto your model."
      ],
      "metadata": {
        "id": "Ey2Ep2LAoRCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title code for metric class\n",
        "\n",
        "class Evaluator(object):\n",
        "    def __init__(self, num_class):\n",
        "        self.num_class = num_class\n",
        "        self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "    def Pixel_Accuracy(self):\n",
        "        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        return Acc\n",
        "\n",
        "    def Pixel_Accuracy_Class(self):\n",
        "        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
        "        Acc = np.nanmean(Acc)\n",
        "        return Acc\n",
        "\n",
        "    def Mean_Intersection_over_Union(self):\n",
        "        MIoU = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "        MIoU = np.nanmean(MIoU)\n",
        "        return MIoU\n",
        "\n",
        "    def Frequency_Weighted_Intersection_over_Union(self):\n",
        "        freq = np.sum(self.confusion_matrix, axis=1) / np.sum(self.confusion_matrix)\n",
        "        iu = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "\n",
        "        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        return FWIoU\n",
        "\n",
        "    def _generate_matrix(self, gt_image, pre_image):\n",
        "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "        count = np.bincount(label, minlength=self.num_class**2)\n",
        "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "        return confusion_matrix\n",
        "\n",
        "    def add_batch(self, gt_image, pre_image):\n",
        "        assert gt_image.shape == pre_image.shape\n",
        "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_class,) * 2)"
      ],
      "metadata": {
        "id": "MLovzg7NngXX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Assignment Part 3\n",
        "\n",
        "Complete the code to undertake model evaluation below. Evaluate twice: once for each model trained with a different loss function.  "
      ],
      "metadata": {
        "id": "kux73Vkuy6bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add the code for evaluation here\n",
        "\n",
        "def do_accuracy_evaluation(model, dataloader, num_classes):\n",
        "    evaluator = Evaluator(num_classes)\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # add batch to evaluator\n",
        "            evaluator.add_batch(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "    # calculate evaluation metrics\n",
        "    pixel_accuracy = evaluator.Pixel_Accuracy()\n",
        "    mean_accuracy = evaluator.Pixel_Accuracy_Class()\n",
        "    mean_IoU = evaluator.Mean_Intersection_over_Union()\n",
        "    frequency_weighted_IoU = evaluator.Frequency_Weighted_Intersection_over_Union()\n",
        "\n",
        "    print(\"pixel accuracy is: \", pixel_accuracy)\n",
        "    print(\"mean accuracy is: \", mean_accuracy)\n",
        "    print(\"mean IoU is: \", mean_IoU)\n",
        "    print(\"frequency weighted IoU is: \", frequency_weighted_IoU)\n",
        "\n",
        "    return pixel_accuracy, mean_accuracy, mean_IoU, frequency_weighted_IoU\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "QYZ4CIs5no54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Demonstrate evaluation of model 1\n",
        "model1 = Unet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "model1.load_state_dict(torch.load('/content/gdrive/MyDrive/A4/Test1/A4_1.pth'))\n",
        "test1 = do_accuracy_evaluation(model1.cuda(), val_loader, 2)"
      ],
      "metadata": {
        "id": "W4WK7HeazNVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Demonstrate evaluation of model 2\n",
        "#@title Demonstrate evaluation of model 1\n",
        "model1 = Unet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "model1.load_state_dict(torch.load('/content/gdrive/MyDrive/A4/Test1/A4_2.pth'))\n",
        "test1 = do_accuracy_evaluation(model1.cuda(), val_loader, 2)"
      ],
      "metadata": {
        "id": "7wKsdVoXzWdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizing activation maps and learned kernels from intermidiate layers in the network**"
      ],
      "metadata": {
        "id": "_6tINf0Xo0vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the multispectral satellite image\n",
        "with rasterio.open(\"/content/gdrive/MyDrive/A4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip1.tif\") as dataset:\n",
        "    # Read the image data as a numpy array and reshape to HWC\n",
        "    image = dataset.read().transpose([1, 2, 0])\n",
        "\n",
        "# Normalize the image data to be between 0 and 1\n",
        "image = do_normalization(image)\n",
        "\n",
        "# Convert the image to a PyTorch tensor and add a batch dimension\n",
        "image_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0)"
      ],
      "metadata": {
        "id": "LS1rKkW3pEdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model and load the pre-trained weights\n",
        "model = Unet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/A4/Test2/Model1/A4_1.pth'))\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "n4H0lHuj7sRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook"
      ],
      "metadata": {
        "id": "OaE5oUzc7wST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the activation map for a particular layer\n",
        "\n",
        "layer_index = 2  # Choose the index of the layer to visualize (0-indexed)\n",
        "layer_name = f'conv{layer_index + 1}'  # Layer name for displaying in the plot title\n",
        "activation = {}"
      ],
      "metadata": {
        "id": "HN8GqQIG7z5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "model.conv1.register_forward_hook(get_activation(layer_name))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model(image_tensor)\n",
        "\n",
        "activation = activation.squeeze().numpy()\n",
        "\n",
        "plt.imshow(activation[layer_index])\n",
        "plt.title(f'Activation Map for {layer_name}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7y5mHSDm78xQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}